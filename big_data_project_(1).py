# -*- coding: utf-8 -*-
"""Big_Data_Project (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R4CeTtB8DNdKvmA9D_gRGOsW2Kc-nx07
"""

# pip install pyspark

import os
from pyspark.sql import SparkSession

# Ensure the log directory exists
log_dir = "/content/spark-logs"
if not os.path.exists(log_dir):
    os.makedirs(log_dir)

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("test") \
    .config("spark.eventLog.enabled", "true") \
    .config("spark.eventLog.dir", f"file://{log_dir}") \
    .getOrCreate()

"""## **Data Exploration**

>Read data & show the first 5 rows
"""

from pyspark.sql.functions import *
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml import Pipeline
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import plotly.express as px

# 1. Load the datasets
train_data = spark.read.csv(r"/content/churn-bigml-20.csv", header=True, inferSchema=True)
test_data = spark.read.csv(r"/content/churn-bigml-80.csv", header=True, inferSchema=True)

# 2. Concatenate train and test data
full_data = train_data.union(test_data)

full_data.show(5)

full_data.count()

full_data.show()

full_data.filter(full_data['Account length'] > 220 ).show()

full_data['Account length'].ma

full_data.groupBy('state').agg(avg('Account length').alias('avg per state')).show()

"""> Shape of Data"""

# Get the number of rows and columns
num_rows = full_data.count()
num_cols = len(full_data.columns)
print(f"The number of rows (Observations): {num_rows}")
print(f"The number of columns (Features): {num_cols}")

"""> Show the schema & know the type of each column"""

full_data.printSchema()

"""> Show missing & Duplication"""

# 1. Check for duplicate rows
print("Duplicate Rows:")
duplicates = full_data.groupBy(full_data.columns).count().filter(col('count') > 1)
duplicates.show(truncate=False)

# Count the number of duplicates
num_duplicates = duplicates.count()
print(f"Number of duplicate rows: {num_duplicates}")

# 2. Check for missing values
print("Missing Values per Column:")
missing_counts = full_data.select([count(when(col(c).isNull(), c)).alias(c) for c in full_data.columns])
missing_counts.show()

"""> ### ***descriptive statistics***"""

full_data.describe().show()

"""# **Data Analysis (EDA) and Visualization**

> # seperate the numerical columm & categorical column
"""

# 1. Identify categorical and numerical columns
# Categorical columns: string or boolean types
cat_columns = [col for col, dtype in full_data.dtypes if dtype in ['string', 'boolean']]
# Numerical columns: int or double types
numerical_columns = [col for col, dtype in full_data.dtypes if dtype in ['int', 'double']]

print(f"Categorical columns >> {cat_columns} <<")
print(f"Numerical columns >> {numerical_columns} <<")

""">

## Define Value counts function
"""

def value_counts(data, column):
    total_count = data.count()

    value_counts_df = (
        data.groupBy(column)
        .agg(count(column).alias("Count"))
        .withColumn("Percentage (%)", expr(f"(Count / {total_count}) * 100"))
        .orderBy("Count", ascending=False)
    )

    return value_counts_df




for col in cat_columns:
    print(f"The {col} Value Counts:")
    value_counts(full_data, col).show()

full_data = full_data.withColumn(
    "Churn_1_0",
    expr("CASE WHEN Churn = true THEN 1 ELSE 0 END")
)

"""## avg_service_calls by churn International plan"""

service_calls_intl_plan = (
    full_data.groupBy("International plan")
    .agg(avg("Customer service calls").alias("Avg Service Calls"))
    .orderBy("Avg Service Calls", ascending=False)
)

service_calls_intl_plan.show()

"""## show the call distribution Minutes"""

call_distribution = (
    full_data.agg(
        sum("Total day minutes").alias("Total Day Minutes"),
        sum("Total eve minutes").alias("Total Evening Minutes"),
        sum("Total night minutes").alias("Total Night Minutes"),
        sum("Total intl minutes").alias("Total International Minutes")
    )
)

call_distribution.show()

"""## churn_percentage for each state"""

from pyspark.sql import functions as F

state_churn = (
    full_data.groupBy("State")
    .agg((F.avg("Churn_1_0")).alias("Churn Rate"))
    .orderBy("Churn Rate", ascending=False)
)

state_churn_percentage = state_churn.withColumn("Churn Rate (%)", F.col("Churn Rate") * 100)

state_churn_percentage.show(10)

"""## customer_service_calls_stats based on churn statues"""

customer_service_calls_stats = (
    full_data.groupBy("Churn")
    .agg(
        F.mean("Customer service calls").alias("Mean Customer Service Calls"),
        F.max("Customer service calls").alias("Max Customer Service Calls"),
        F.min("Customer service calls").alias("Min Customer Service Calls")
    )
)
customer_service_calls_stats.show()

"""## Churn Rate by Customer service calls"""

from pyspark.sql.functions import avg, col
# GroupBy and aggregation to compute churn rate
cust_service_churn = (
    full_data.groupBy("Customer service calls")
    .agg(
        round(avg(col("Churn_1_0")) * 100, 2).alias("Churn (%)")
    )
    .orderBy("Churn (%)", ascending=False)
)
cust_service_churn.show()

"""# Data Visualization

### ***distribution of numerical features***
"""

# Define numerical columns (assumed based on prior importantColumns)
numerical_columns = [
    'Customer service calls',
    'Total day minutes',
    'Total night minutes',
    'Total eve minutes',
    'Total intl calls',
    'Number vmail messages',
    'Account length'
]

# Select only numerical columns to minimize data transfer
full_data_subset = full_data.select(numerical_columns)

# Convert to Pandas DataFrame
full_data_pd = full_data_subset.toPandas()

# Set the seaborn style with dark grid
sns.set_style("darkgrid", {"grid.color": ".6", "grid.linestyle": ":"})

# Create subplots
fig, axes = plt.subplots(len(numerical_columns), 1, figsize=(10, 55), facecolor="lightgray")

# Plot histograms
for i, col in enumerate(numerical_columns):
    axes[i].hist(full_data_pd[col], bins=20, edgecolor='black', color='skyblue')
    axes[i].set_title(f'Distribution of {col}', fontweight='bold')
    axes[i].set_xlabel(col, fontsize=11)
    axes[i].set_ylabel('Frequency', fontsize=11)

# Add the suptitle
fig.suptitle('Distribution Analysis of Numerical Features for Churn Analysis',
             fontsize=16, fontweight='bold', y=0.98)

# Adjust layout to make space for the suptitle
plt.tight_layout(rect=[0, 0, 1, 0.97])

# Show the plot
plt.show()

"""> ## ***distribution of Categorical data***"""

# Define categorical columns (excluding State for clarity)
cat_columns = ['Churn','International plan', 'Voice mail plan']

# Convert full data to Pandas for Seaborn countplot (alternative to aggregation)
full_data_pd = full_data.select(cat_columns).toPandas()

# Create subplots
fig, axes = plt.subplots(1, len(cat_columns), figsize=(25, 12), facecolor="lightgray")

# Plot horizontal countplots
for i, column in enumerate(cat_columns):
    ax = axes[i]
    sns.countplot(
        data=full_data_pd,
        x=column,
        ax=ax,
        palette=["#778da9", "cyan"],
        order=full_data_pd[column].value_counts().index
    )
    ax.set_title(column, size=18)
    ax.set_xlabel('Count', fontsize=12)
    ax.set_ylabel(column, fontsize=12)
    # Add value labels on bars
    for p in ax.patches:
        ax.annotate(
            f'{int(p.get_width())}',
            (p.get_width(), p.get_y() + p.get_height() / 2),
            xytext=(5, 0),
            textcoords='offset points',
            va='center'
        )

# Add suptitle
fig.suptitle('Distribution of Categorical Features for Churn Analysis', fontsize=16, fontweight='bold')

# Adjust layout
plt.tight_layout()

# Show the plot
plt.show()

"""### ***Numerical Features mean by Churn Status***

"""

importantColumns = [
    'Customer service calls',
    'Total day minutes',
    'Total night minutes',
    'Total eve minutes',
    'Total intl calls',
    'Number vmail messages',
    'Account length'
]

# Select only Churn and important columns to minimize data transfer
selected_columns = ['Churn'] + importantColumns
full_data_subset = full_data.select(selected_columns)

# Convert to Pandas DataFrame
full_data_pd = full_data_subset.toPandas()

# Create subplot layout
fig, axes = plt.subplots(len(importantColumns), 1, figsize=(8, 30))

# Plot barplots for each important column
for i, col in enumerate(importantColumns):
    sns.barplot(data=full_data_pd, x='Churn', y=col, ax=axes[i], palette='GnBu')
    axes[i].set_title(f'Mean of {col} by Churn', fontsize=14)
    axes[i].set_xlabel('Churn', fontsize=12)
    axes[i].set_ylabel(col, fontsize=12)

# Add overall title
fig.suptitle('Comparison of Key Features by Churn Status', fontsize=18, y=0.98)

# Adjust layout
plt.tight_layout()
plt.subplots_adjust(top=0.95)
plt.show()

# Compute churn rate by State
state_churn = (
    full_data.groupBy("State")  # Assuming column is 'State', not 'State Name'
    .agg(round(avg(col("Churn_1_0")), 4).alias("Churn Rate"))
    .orderBy("Churn Rate", ascending=True)
    .limit(10)
)

# Convert to Pandas
state_churn_pd = state_churn.toPandas()

# Create the bar chart using Plotly Express
fig = px.bar(
    state_churn_pd,
    y='State',
    x='Churn Rate',
    color='Churn Rate',
    color_continuous_scale=px.colors.sequential.Viridis,
    title='Churn Rate by State'
)

# Display the chart
fig.show()

## *** Data Preprocessing: Encoding, Scaling & Splitting The Data***

full_data = full_data.drop('Total day minutes bin')

# Plotting the correlation heatmap for numerical features
pandas_df = full_data.select(numerical_columns).toPandas()
corr_matrix = pandas_df.corr()
plt.figure(figsize=(20, 15))
sns.heatmap(corr_matrix, annot=True)
plt.title("Correlation Heatmap of Numerical Features")
plt.show()

## **Encoding**

from pyspark.sql.functions import when, expr

full_data = full_data.withColumn(
    "Churn",
    when(full_data["Churn"] == True, 1).otherwise(0)
)

cat_cols = ['State', 'International plan', 'Voice mail plan']
indexers = [StringIndexer(inputCol=col, outputCol=col + "_idx") for col in cat_cols]
label_indexer = StringIndexer(inputCol="Churn", outputCol="label")
features = [col for col in full_data.columns if col not in cat_cols + ['Churn']] + [col + "_idx" for col in cat_cols]
assembler = VectorAssembler(inputCols=features, outputCol="features")
pipeline = Pipeline(stages=indexers + [label_indexer, assembler])

# Split into train and test sets
full_data = pipeline.fit(full_data).transform(full_data)
train_data, test_data = full_data.randomSplit([0.8, 0.2], seed=42)

### Label Encoding for Categorical Features

from pyspark.ml.feature import StringIndexer

# Create StringIndexer objects
indexer_intl_plan = StringIndexer(inputCol="International plan", outputCol="International plan_encoded")
indexer_voice_mail_plan = StringIndexer(inputCol="Voice mail plan", outputCol="Voice mail plan_encoded")

# Fit indexers on training data
intl_model = indexer_intl_plan.fit(train_data)
voice_model = indexer_voice_mail_plan.fit(train_data)

# Transform training data
train_data = intl_model.transform(train_data)
train_data = voice_model.transform(train_data)

# Transform test data using the same fitted models
test_data = intl_model.transform(test_data)
test_data = voice_model.transform(test_data)

## Check data imbalance

churn_counts = full_data.groupBy('Churn').count().toPandas()
plt.figure(figsize=(8, 6))
sns.barplot(x='Churn', y='count', data=churn_counts)
plt.title('Churn Distribution')
plt.xlabel('Churn')
plt.ylabel('Count')
plt.show()

### Handling Class Imbalance using SMOTE

# Before splitting into train/test:
major_df = full_data.filter(col("Churn") == 0)
minor_df = full_data.filter(col("Churn") == 1)
ratio = float(major_df.count() / minor_df.count()) # Change ratio to float

# Oversample the minority class
oversampled_minor_df = minor_df.sample(withReplacement=True, fraction=ratio, seed=42)

# Combine oversampled minority with majority class
full_data_oversampled = major_df.unionAll(oversampled_minor_df)

# split into train/test:
train_data, test_data = full_data_oversampled.randomSplit([0.8, 0.2], seed=42)

pandas_df = train_data.select('Churn').toPandas()
sns.countplot(data=pandas_df, x='Churn')
plt.show()

### Scaling Numerical Features

# Transform train and test data to create the 'numerical_features' column
from pyspark.ml.feature import StandardScaler, VectorAssembler

numerical_columns = ['Account length', 'Number vmail messages', 'Total day minutes', 'Total day calls', 'Total eve minutes', 'Total eve calls', 'Total night minutes', 'Total night calls', 'Total intl minutes', 'Total intl calls', 'Customer service calls']
assembler = VectorAssembler(inputCols=numerical_columns, outputCol="numerical_features")

train_data = assembler.transform(train_data)
test_data = assembler.transform(test_data)

scaler = StandardScaler(inputCol="numerical_features", outputCol="scaled_features", withStd=True, withMean=True)
scalerModel = scaler.fit(train_data)

train_data = scalerModel.transform(train_data)
test_data = scalerModel.transform(test_data)

### Correlation Matrix Including Target

pandas_df = train_data.select(numerical_columns + ['Churn']).toPandas()
corr_matrix = pandas_df.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, linewidths=0.5)
plt.title('Correlation Matrix Including Target (Churn)')
plt.show()

# RandomForestClassifier

from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.mllib.evaluation import MulticlassMetrics

rf = RandomForestClassifier(labelCol="label", featuresCol="scaled_features", numTrees=100)

model = rf.fit(train_data)

predictions = model.transform(test_data)

predictionAndLabels = predictions.select("prediction", "label").rdd

metrics = MulticlassMetrics(predictionAndLabels)
print("Classification Report:")
print(f"Accuracy: {metrics.accuracy}")
print(f"Precision: {metrics.weightedPrecision}")
print(f"Recall: {metrics.weightedRecall}")
print(f"F1 Score: {metrics.weightedFMeasure}")

conf_matrix = metrics.confusionMatrix().toArray()
conf_matrix_df = pd.DataFrame(conf_matrix)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_df, annot=True,cmap="Blues", fmt=".0f")
plt.title("Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# LogisticRegression

from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.mllib.evaluation import MulticlassMetrics

lr = LogisticRegression(featuresCol="scaled_features", labelCol="label", maxIter=100, regParam=0.1, elasticNetParam=0.0)
lr_model = lr.fit(train_data)


lr_predictions = lr_model.transform(test_data)

# Evaluate the model
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
lr_accuracy = evaluator.evaluate(lr_predictions)
print(f"Logistic Regression Accuracy: {lr_accuracy}")

# Confusion Matrix and Classification Report
predictionAndLabels = lr_predictions.select("prediction", "label").rdd
metrics = MulticlassMetrics(predictionAndLabels)

print("Confusion Matrix:")
print(metrics.confusionMatrix())

print("Classification Report:")
print(f"Accuracy: {metrics.accuracy}")
print(f"Precision: {metrics.weightedPrecision}")
print(f"Recall: {metrics.weightedRecall}")
print(f"F1 Score: {metrics.weightedFMeasure}")


conf_matrix = metrics.confusionMatrix().toArray()
conf_matrix_df = pd.DataFrame(conf_matrix)

plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix_df, annot=True, cmap="Blues", cbar=False, fmt=".0f",
                xticklabels=['Predicted 0', 'Predicted 1'],
                yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix for Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

#DecisionTreeClassifier

from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.mllib.evaluation import MulticlassMetrics

dt = DecisionTreeClassifier(featuresCol="scaled_features", labelCol="label", maxBins=52)
dt_model = dt.fit(train_data)

dt_predictions = dt_model.transform(test_data)

evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
dt_accuracy = evaluator.evaluate(dt_predictions)
print(f"Decision Tree Accuracy: {dt_accuracy}")

# Confusion Matrix and Classification Report
predictionAndLabels = dt_predictions.select("prediction", "label").rdd
metrics = MulticlassMetrics(predictionAndLabels)

print("Confusion Matrix:")
print(metrics.confusionMatrix())

print("Classification Report:")
print(f"Accuracy: {metrics.accuracy}")
print(f"Precision: {metrics.weightedPrecision}")
print(f"Recall: {metrics.weightedRecall}")
print(f"F1 Score: {metrics.weightedFMeasure}")

# Plot Confusion Matrix
conf_matrix = metrics.confusionMatrix().toArray()
conf_matrix_df = pd.DataFrame(conf_matrix)

plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix_df, annot=True, cmap="Blues", cbar=False,fmt=".0f",
                xticklabels=['Predicted 0', 'Predicted 1'],
                yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix for Decision Tree')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

